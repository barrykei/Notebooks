{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Science Practice Notebook\n",
        "\n",
        "This notebook covers common data science tasks and workflows using Python.\n",
        "\n",
        "## Topics Covered:\n",
        "1. Data Loading and Exploration\n",
        "2. Data Cleaning and Preprocessing\n",
        "3. Exploratory Data Analysis (EDA)\n",
        "4. Data Visualization\n",
        "5. Statistical Analysis\n",
        "6. Machine Learning Basics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import essential libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Exploration\n",
        "\n",
        "Let's create a sample dataset for practice. In real scenarios, you'd load data from CSV, Excel, databases, or APIs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a sample dataset with some realistic patterns\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "# Generate synthetic data\n",
        "data = {\n",
        "    'age': np.random.randint(18, 80, n_samples),\n",
        "    'income': np.random.normal(50000, 15000, n_samples),\n",
        "    'education_years': np.random.randint(12, 20, n_samples),\n",
        "    'experience_years': np.random.randint(0, 40, n_samples),\n",
        "    'city': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'], n_samples),\n",
        "    'satisfaction_score': np.random.uniform(1, 10, n_samples)\n",
        "}\n",
        "\n",
        "# Create relationship: income increases with experience and education\n",
        "data['income'] = data['income'] + data['experience_years'] * 2000 + data['education_years'] * 1500\n",
        "\n",
        "# Add some noise and missing values\n",
        "data['income'] = data['income'] + np.random.normal(0, 5000, n_samples)\n",
        "data['income'] = np.abs(data['income'])  # Ensure positive values\n",
        "\n",
        "# Introduce missing values (5% of income data)\n",
        "missing_indices = np.random.choice(n_samples, size=int(n_samples * 0.05), replace=False)\n",
        "data['income'][missing_indices] = np.nan\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(\"Sample dataset created!\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic information about the dataset\n",
        "print(\"Dataset Info:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"\\nColumn names: {list(df.columns)}\")\n",
        "print(f\"\\nData types:\\n{df.dtypes}\")\n",
        "print(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
        "print(f\"\\nBasic statistics:\\n{df.describe()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Cleaning and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle missing values\n",
        "print(\"Before handling missing values:\")\n",
        "print(f\"Missing values in income: {df['income'].isnull().sum()}\")\n",
        "\n",
        "# Option 1: Fill with median (good for numerical data)\n",
        "df['income'].fillna(df['income'].median(), inplace=True)\n",
        "\n",
        "# Option 2: Drop rows (if few missing values)\n",
        "# df = df.dropna()\n",
        "\n",
        "print(\"\\nAfter handling missing values:\")\n",
        "print(f\"Missing values in income: {df['income'].isnull().sum()}\")\n",
        "\n",
        "# Check for duplicates\n",
        "print(f\"\\nDuplicate rows: {df.duplicated().sum()}\")\n",
        "\n",
        "# Remove duplicates if any\n",
        "df = df.drop_duplicates()\n",
        "print(f\"Shape after removing duplicates: {df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle outliers (using IQR method)\n",
        "def detect_outliers_iqr(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
        "\n",
        "# Check for outliers in income\n",
        "outliers = detect_outliers_iqr(df, 'income')\n",
        "print(f\"Number of outliers in income: {len(outliers)}\")\n",
        "print(f\"Outlier percentage: {len(outliers)/len(df)*100:.2f}%\")\n",
        "\n",
        "# Option: Remove or cap outliers (capping is often better)\n",
        "# For demonstration, we'll cap outliers\n",
        "Q1 = df['income'].quantile(0.25)\n",
        "Q3 = df['income'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "df['income'] = df['income'].clip(lower=lower_bound, upper=upper_bound)\n",
        "print(f\"\\nIncome range after capping: ${df['income'].min():.2f} - ${df['income'].max():.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Exploratory Data Analysis (EDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation analysis\n",
        "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
        "correlation_matrix = df[numeric_columns].corr()\n",
        "\n",
        "print(\"Correlation Matrix:\")\n",
        "print(correlation_matrix)\n",
        "\n",
        "# Visualize correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
        "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Correlation Matrix of Numerical Variables')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of numerical variables\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, col in enumerate(numeric_columns):\n",
        "    axes[i].hist(df[col].dropna(), bins=30, edgecolor='black', alpha=0.7)\n",
        "    axes[i].set_title(f'Distribution of {col}')\n",
        "    axes[i].set_xlabel(col)\n",
        "    axes[i].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Categorical variable analysis\n",
        "print(\"City distribution:\")\n",
        "print(df['city'].value_counts())\n",
        "print(\"\\nPercentage distribution:\")\n",
        "print(df['city'].value_counts(normalize=True) * 100)\n",
        "\n",
        "# Visualize categorical data\n",
        "plt.figure(figsize=(10, 6))\n",
        "df['city'].value_counts().plot(kind='bar', color='steelblue', edgecolor='black')\n",
        "plt.title('Distribution of Cities')\n",
        "plt.xlabel('City')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scatter plot: Income vs Experience\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(df['experience_years'], df['income'], alpha=0.5, color='steelblue')\n",
        "plt.xlabel('Experience Years')\n",
        "plt.ylabel('Income')\n",
        "plt.title('Income vs Experience Years')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Box plot: Income by City\n",
        "plt.figure(figsize=(10, 6))\n",
        "df.boxplot(column='income', by='city', ax=plt.gca())\n",
        "plt.title('Income Distribution by City')\n",
        "plt.suptitle('')  # Remove default title\n",
        "plt.xlabel('City')\n",
        "plt.ylabel('Income')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pair plot for key numerical variables\n",
        "key_vars = ['age', 'income', 'education_years', 'experience_years']\n",
        "sns.pairplot(df[key_vars], diag_kind='kde', plot_kws={'alpha': 0.6})\n",
        "plt.suptitle('Pair Plot of Key Variables', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Statistical Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical summary\n",
        "print(\"Statistical Summary for Income:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Mean: ${df['income'].mean():.2f}\")\n",
        "print(f\"Median: ${df['income'].median():.2f}\")\n",
        "print(f\"Standard Deviation: ${df['income'].std():.2f}\")\n",
        "print(f\"Minimum: ${df['income'].min():.2f}\")\n",
        "print(f\"Maximum: ${df['income'].max():.2f}\")\n",
        "print(f\"Skewness: {df['income'].skew():.2f}\")\n",
        "print(f\"Kurtosis: {df['income'].kurtosis():.2f}\")\n",
        "\n",
        "# Group statistics\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Average Income by City:\")\n",
        "print(\"=\" * 50)\n",
        "city_income = df.groupby('city')['income'].agg(['mean', 'median', 'std'])\n",
        "print(city_income)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hypothesis testing example: Compare income between two cities\n",
        "from scipy import stats\n",
        "\n",
        "ny_income = df[df['city'] == 'New York']['income']\n",
        "la_income = df[df['city'] == 'Los Angeles']['income']\n",
        "\n",
        "# Two-sample t-test\n",
        "t_stat, p_value = stats.ttest_ind(ny_income, la_income)\n",
        "\n",
        "print(\"Hypothesis Test: Income difference between NY and LA\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"NY Average Income: ${ny_income.mean():.2f}\")\n",
        "print(f\"LA Average Income: ${la_income.mean():.2f}\")\n",
        "print(f\"T-statistic: {t_stat:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"\\nResult: Significant difference (p < 0.05)\")\n",
        "else:\n",
        "    print(\"\\nResult: No significant difference (p >= 0.05)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Machine Learning Basics\n",
        "\n",
        "Let's build a simple linear regression model to predict income based on other features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for modeling\n",
        "# Select features and target\n",
        "features = ['age', 'education_years', 'experience_years']\n",
        "target = 'income'\n",
        "\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# Encode categorical variables if needed (using one-hot encoding)\n",
        "city_encoded = pd.get_dummies(df['city'], prefix='city')\n",
        "X = pd.concat([X, city_encoded], axis=1)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape}\")\n",
        "print(f\"Test set size: {X_test.shape}\")\n",
        "print(f\"\\nFeatures: {list(X.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale features (important for many ML algorithms)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert back to DataFrame for better readability\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "print(\"Features scaled successfully!\")\n",
        "print(f\"Training set mean: {X_train_scaled.mean().values}\")\n",
        "print(f\"Training set std: {X_train_scaled.std().values}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = model.predict(X_train_scaled)\n",
        "y_test_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate model\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(\"Model Performance:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Training RMSE: ${train_rmse:.2f}\")\n",
        "print(f\"Test RMSE: ${test_rmse:.2f}\")\n",
        "print(f\"Training R²: {train_r2:.4f}\")\n",
        "print(f\"Test R²: {test_r2:.4f}\")\n",
        "\n",
        "# Feature importance (coefficients)\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Feature Coefficients:\")\n",
        "print(\"=\" * 50)\n",
        "coefficients = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Coefficient': model.coef_\n",
        "}).sort_values('Coefficient', ascending=False)\n",
        "print(coefficients)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions vs actual values\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Training set\n",
        "axes[0].scatter(y_train, y_train_pred, alpha=0.5, color='steelblue')\n",
        "axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
        "axes[0].set_xlabel('Actual Income')\n",
        "axes[0].set_ylabel('Predicted Income')\n",
        "axes[0].set_title(f'Training Set (R² = {train_r2:.4f})')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Test set\n",
        "axes[1].scatter(y_test, y_test_pred, alpha=0.5, color='steelblue')\n",
        "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "axes[1].set_xlabel('Actual Income')\n",
        "axes[1].set_ylabel('Predicted Income')\n",
        "axes[1].set_title(f'Test Set (R² = {test_r2:.4f})')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps for Practice\n",
        "\n",
        "1. **Try different datasets**: Load real datasets from Kaggle, UCI ML Repository, or APIs\n",
        "2. **Feature engineering**: Create new features, transform variables, handle categorical data\n",
        "3. **Advanced visualizations**: Create interactive plots with Plotly, customize matplotlib styles\n",
        "4. **More ML models**: Try Random Forest, XGBoost, Neural Networks\n",
        "5. **Model evaluation**: Cross-validation, hyperparameter tuning, model comparison\n",
        "6. **Time series analysis**: If working with temporal data\n",
        "7. **Text analysis**: Natural Language Processing tasks\n",
        "8. **Deep learning**: Use TensorFlow or PyTorch for complex models\n",
        "\n",
        "## Tips for Using Cursor with Jupyter Notebooks\n",
        "\n",
        "- Use Cursor's AI to explain code, generate visualizations, or debug errors\n",
        "- Ask for code suggestions when stuck on a specific task\n",
        "- Use Cursor to refactor and optimize your code\n",
        "- Generate documentation and markdown cells for your analysis\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
